{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Word Mover Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "    1) Executive Summary\n",
    "    2) Data\n",
    "    3) TensorFlow\n",
    "    4) MCWMD\n",
    "    5) Tuning Hyperparameters\n",
    "    6) Naive Bayes\n",
    "    7) Wish List\n",
    "    8) Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary\n",
    "\n",
    "Monte Carlo Word Mover Distance is a combination of monte carlo techniques, word2vec and word mover's distance(aka earth mover's distance applied to words) to classify text.\n",
    "\n",
    "Monte Carlo Word Mover Distance starts by using monte carlo techniques to generates a representative set of text for each potential label. Then MCWMD attempts to predict a given text's label by finding the minimum cost of transforming the given text into the generated representative texts.  Whichever label's representative set of texts has the lowest transformation cost, on average, will become the label for the given text.\n",
    "\n",
    "The culmination of these steps are attempting to map text to the label where the text associated with that label have similar word usage and use words with close to the same meaning (see secton 4 for more in depth description).\n",
    "\n",
    "I tested MCWMD on Amazon movie reviews.  I tested the algorithm on the top 100 most helpful reviews of rating 1 and rating 5.  MCWMD showed some promise in having a inuitive explanation and a decent accuracy rate, but requires parallelization if it is ever to be executed outside of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 5 Rating:\n",
    "\"I love Chinese food, but have been intimidated by the thought of cooking much of it because I didn't always feel like I understood the technique.  Even a cookbook with pictures can only show you so much.  Having this show to watch really made the obscure seem clear & I now have a lot more confidence.  Plus, everything looks super tasty!\"\n",
    "\n",
    "### 1 Rating:\n",
    "\"WOW - this show was so bad it's hard to know where to begin.  If you like unsympathetic characters, lots of swearing, gratuitous everything, and complete incoherence, then maybe this show is for you.  Otherwise, consider the following, because I watched this so you don't have to.We start, basically, with a small group of people who all get on an elevator and it gets stuck, so they are now apparently going to bond for life.  There has been some sort of unspecified disaster.  They never show any of it, supposedly because it's all mysterious and all, ok, I get that.  But what kind of disaster makes everyone completely incompetent?  The day AFTER this supposed disaster has started, there are people rushing - not merely milling, mind you, but RUSHING around the streets.  In every direction, too.  There are a LOT of first responders on the scene - apparently none of them have any training in crowd control, because every cop, EMT, or fire fighter we see is not doing anything except walking around.  (Except for Officer Munoz, but she's going to get her own section here shortly, so hang on.)  There's a school bus full of crying children, banging on the windows, and everyone just walks on past them - a day after the disaster has started, and no one has gotten the kids off the bus?  Seriously, in that amount of time, the kids would have just gotten off the bus themselves.  NO ONE in this show acts the way normal people would act, about ANYTHING.  At one point, Our Heroes are trying to escape the city in an ambulance that Dee, the African-American escaped prisoner (and one of only three people who seem to have names) stole.  It is broad daylight, there are first responders and first responder vehicles EVERYWHERE, yet as soon as Our Heroes turn onto the street, the crowd turns to look at them like they have never seen an ambulance before in their lives.  They quickly surround the ambulance and start trying to tip it over, because, yeah, tipping ambulances is the first thing *I* think about in a survival situation.  A quick camera cut to ambulance interior, where Our Heroes decide to gun it to get out of there.  Quick camera cut back - the crowd surrounding the ambulance is GONE - well, ok, everyone is suddenly a nice safe 10 feet back so the ambulance can gun it without running over anyone.  Good thinking, extras!  Don't want to get anyone hurt.  Or, more likely, don't want to pay for any stunts for this low-budget disaster.They take the ambulance to Old Lady Plot Device's Bel Air Mansion (which, by the way, apparently opens has a primeval forest in the back), where the Easily Sexually Exploited Character decides to disrobe and swim in the pool.  'Cuz, y'know, that's always the first thing you want to do in a survival situation.  Never mind that the other people in our group include Skeezy Lawyer who is currently sexually exploiting her, Dee the escaped prisoner, and Drunken Swearing Irishman.  (Hey, don't blame me, I didn't put all these racist stereotypes in the show, I'm just pointing them out.)  And it's not like she feels so safe because the other people in the group are so nice and protective - the only non-criminal-seeming guy is a clown, and the only woman who seems like she might be able to protect anyone is Officer Munoz.Oh, Officer Munoz.  I actually feel so sorry for the actress who has to play this awful character, because she was one of the few who could actually act, but they aren't giving her anything at ALL to work with.  Here is a list of everything I can remember (I'm sure I forgot a few things) that Officer Munoz does WRONG -- When Old Lady Plot Device announces she has a medical condition, Officer Munoz never asks what it is, so when Old Lady Plot Device keels over, everyone has to play Charades to figure out she is diabetic.- Officer Munoz never asks anyone's name.  Several hours into their journey, when they've all had at least a long ambulance ride out to Bel Air where they could have taken the time to get acquainted, she refers to the clown as 'Clown' and he has to point out that his name is Dave.  To be fair, everyone in this show is incredibly unconcerned with names.  It's like taking my little boy to the playground, he'll play with some other kid for four hours, and then I'll say, &#34;So, what's your friend's name?&#34; and he'll look at me like why the heck would THAT matter.  It's just not usually an attitude you expect adults to take.  Especially cops.- Officer Munoz gut-shoots an unarmed man for running towards her.  She then leaves him on the parking garage floor and watches him bleed out, never offering any the most rudimentary first aid.- When Our Heroes try to escape in the ambulance, Officer Munoz, instead of DRIVING the silly thing, or at least sitting in the front seat so it looks legit, decides to hang off the back door.  When the crowd starts rocking the ambulance, she doesn't do anything except holler.- Later, at the Bel Air mansion, a group of Hispanic criminals come to loot the mansion.  Officer Munoz is apparently helpless against the magical powers of the Spanish language, because she literally stands there silently and lets one of these guys walk slowly up to her and, apparently because he is insulting her in Spanish, she lets him take her gun RIGHT OUT OF HER HAND.  And she doesn't even step back, or flinch, or anything.  Good thing we have those aforementioned racial stereotypes to get us out of this situation, because Dee the African-American escaped prisoner is considerably more competent with a gun and starts shooting so Our Heroes have a chance to run away.Oh, and this running away needs a little more detail, too.  Our Heroes all run back into the mansion, which has iron-barred doors, so everyone's safe now, right?  Well, not if you don't SHUT THE DOOR!  Yes - they all decided, apparently without ANY discussion, that the best course of action was to run all the way through the mansion to the back door, and run out into the primeval forest in Old Lady Plot Device's backyard.  If only they had thought to SHUT THE DOOR the bad guys would have at LEAST had to run all the way AROUND the house to catch them.One final thing & then I have to go lie down with a cold cloth on my forehead.  The pilot ends with everyone discovering they all have the same birthday, and French Actress Character is attacked in the woods by Super Fast Alien Spider Demon Guy.  Dee shoots the Spider Alien Demon, it says 'Alea iacta est' twice ('the die is cast' for those of you who aren't up on your Latin), and skitters off into the woods.  I gather this is supposed to intrigue me enough for me to overlook all the horrible inconsistencies of the rest of the hour and tune in for  the next episode.  It didn't.  I had MUCH more fun writing this review than I did watching the show, though, so at least I got something out of it.  Don't bother yourself with it, though.\" \n",
    "\n",
    "\n",
    "### Author\n",
    "\n",
    "Both of these reviews, although appearing to be from different authors, are both complements of Denise Patterson.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "What is Denise Telling Us?\n",
    "----\n",
    "\n",
    "Despite having the same author, there are obvious differences in word usage and type of words used to describe these two movies.  MCWMD is attempting to leverage these differences in word frequency and usage across labels to classify text, and if we can see these differences even with the same author, maybe there is hope for MCWMD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Rating Distribution\n",
    "----\n",
    "\n",
    "![](./Images/RatingDistrib.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Average Review Length\n",
    "----\n",
    "\n",
    "![](./Images/AvgReviewLength.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Unique Words\n",
    "----\n",
    "\n",
    "![](./Images/UniqueWords.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Average Unique Words\n",
    "----\n",
    "\n",
    "![](./Images/AvgUniqueWords.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Top Words\n",
    "----\n",
    "\n",
    "![](./Images/TopWords.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Data Processing\n",
    "----\n",
    "\n",
    "- Lower Case\n",
    "\n",
    "- Add \"neg_\" to words following a negation\n",
    "\n",
    "- convert strings to list form\n",
    "\n",
    "- Due to MCWMD speed (or lack there of)\n",
    "    - Only use 1 and 5 ratings to train embeddings, train and test MCWMD\n",
    "    - Only use top 100 most helpful reviews (in absolute terms) for each rating type to train and test MCWMD\n",
    "        - OVER FIT \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert reviews to word embeddings via TensorFlow\n",
    "\n",
    "Kept TensorFlow default of top 50,000 words and 128 latent dimensions\n",
    "\n",
    "https://www.tensorflow.org/versions/r0.7/tutorials/word2vec/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copyright 2015 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "#from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "# Step 1: Download the data.\n",
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "# def maybe_download(filename, expected_bytes):\n",
    "#   \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "#   if not os.path.exists(filename):\n",
    "#     filename, _ = urllib.request.urlretrieve(url + filename, filename)\n",
    "#   statinfo = os.stat(filename)\n",
    "#   if statinfo.st_size == expected_bytes:\n",
    "#     print('Found and verified', filename)\n",
    "#   else:\n",
    "#     print(statinfo.st_size)\n",
    "#     raise Exception(\n",
    "#         'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "#   return filename\n",
    "\n",
    "# filename = maybe_download('text8.zip', 31344016)\n",
    "\n",
    "\n",
    "# # Read the data into a list of strings.\n",
    "# def read_data(filename):\n",
    "#   \"\"\"Extract the first file enclosed in a zip file as a list of words\"\"\"\n",
    "#   with zipfile.ZipFile(filename) as f:\n",
    "#     data = f.read(f.namelist()[0]).split()\n",
    "#   return data\n",
    "\n",
    "words = flattextList15\n",
    "print('Data size', len(words))\n",
    "\n",
    "# Step 2: Build the dictionary and replace rare words with UNK token.\n",
    "vocabulary_size = 50000\n",
    "\n",
    "def build_dataset(words):\n",
    "  count = [['UNK', -1]]\n",
    "  count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "  dictionary = dict()\n",
    "  for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "  data = list()\n",
    "  unk_count = 0\n",
    "  for word in words:\n",
    "    if word in dictionary:\n",
    "      index = dictionary[word]\n",
    "    else:\n",
    "      index = 0  # dictionary['UNK']\n",
    "      unk_count += 1\n",
    "    data.append(index)\n",
    "  count[0][1] = unk_count\n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "  return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
    "del words  # Hint to reduce memory.\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10])\n",
    "\n",
    "data_index = 0\n",
    "\n",
    "\n",
    "# Step 3: Function to generate a training batch for the skip-gram model.\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "  global data_index\n",
    "  assert batch_size % num_skips == 0\n",
    "  assert num_skips <= 2 * skip_window\n",
    "  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "  span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "  buffer = collections.deque(maxlen=span)\n",
    "  for _ in range(span):\n",
    "    buffer.append(data[data_index])\n",
    "    data_index = (data_index + 1) % len(data)\n",
    "  for i in range(batch_size // num_skips):\n",
    "    target = skip_window  # target label at the center of the buffer\n",
    "    targets_to_avoid = [ skip_window ]\n",
    "    for j in range(num_skips):\n",
    "      while target in targets_to_avoid:\n",
    "        target = random.randint(0, span - 1)\n",
    "      targets_to_avoid.append(target)\n",
    "      batch[i * num_skips + j] = buffer[skip_window]\n",
    "      labels[i * num_skips + j, 0] = buffer[target]\n",
    "    buffer.append(data[data_index])\n",
    "    data_index = (data_index + 1) % len(data)\n",
    "  return batch, labels\n",
    "\n",
    "batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)\n",
    "for i in range(8):\n",
    "  print(batch[i], '->', labels[i, 0])\n",
    "  print(reverse_dictionary[batch[i]], '->', reverse_dictionary[labels[i, 0]])\n",
    "\n",
    "# Step 4: Build and train a skip-gram model.\n",
    "\n",
    "batch_size = 128\n",
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "skip_window = 1       # How many words to consider left and right.\n",
    "num_skips = 2         # How many times to reuse an input to generate a label.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent.\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "num_sampled = 64    # Number of negative examples to sample.\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "  train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "  # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "  with tf.device('/cpu:0'):\n",
    "    # Look up embeddings for inputs.\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "    # Construct the variables for the NCE loss\n",
    "    nce_weights = tf.Variable(\n",
    "        tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                            stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  # Compute the average NCE loss for the batch.\n",
    "  # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "  # time we evaluate the loss.\n",
    "  loss = tf.reduce_mean(\n",
    "      tf.nn.nce_loss(nce_weights, nce_biases, embed, train_labels,\n",
    "                     num_sampled, vocabulary_size))\n",
    "\n",
    "  # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "  # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "  normalized_embeddings = embeddings / norm\n",
    "  valid_embeddings = tf.nn.embedding_lookup(\n",
    "      normalized_embeddings, valid_dataset)\n",
    "  similarity = tf.matmul(\n",
    "      valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "# Step 5: Begin training.\n",
    "num_steps = 100001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  # We must initialize all variables before we use them.\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "\n",
    "  average_loss = 0\n",
    "  for step in xrange(num_steps):\n",
    "    batch_inputs, batch_labels = generate_batch(\n",
    "        batch_size, num_skips, skip_window)\n",
    "    feed_dict = {train_inputs : batch_inputs, train_labels : batch_labels}\n",
    "\n",
    "    # We perform one update step by evaluating the optimizer op (including it\n",
    "    # in the list of returned values for session.run()\n",
    "    _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "    average_loss += loss_val\n",
    "\n",
    "    if step % 2000 == 0:\n",
    "      if step > 0:\n",
    "        average_loss /= 2000\n",
    "      # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "      print(\"Average loss at step \", step, \": \", average_loss)\n",
    "      average_loss = 0\n",
    "\n",
    "    # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "    if step % 10000 == 0:\n",
    "      sim = similarity.eval()\n",
    "      for i in xrange(valid_size):\n",
    "        valid_word = reverse_dictionary[valid_examples[i]]\n",
    "        top_k = 8 # number of nearest neighbors\n",
    "        nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "        log_str = \"Nearest to %s:\" % valid_word\n",
    "        for k in xrange(top_k):\n",
    "          close_word = reverse_dictionary[nearest[k]]\n",
    "          log_str = \"%s %s,\" % (log_str, close_word)\n",
    "        print(log_str)\n",
    "  final_embeddings = normalized_embeddings.eval()\n",
    "\n",
    "# Step 6: Visualize the embeddings.\n",
    "\n",
    "# def plot_with_labels(low_dim_embs, labels, filename='tsne.png'):\n",
    "#   assert low_dim_embs.shape[0] >= len(labels), \"More labels than embeddings\"\n",
    "#   plt.figure(figsize=(18, 18))  #in inches\n",
    "#   for i, label in enumerate(labels):\n",
    "#     x, y = low_dim_embs[i,:]\n",
    "#     plt.scatter(x, y)\n",
    "#     plt.annotate(label,\n",
    "#                  xy=(x, y),\n",
    "#                  xytext=(5, 2),\n",
    "#                  textcoords='offset points',\n",
    "#                  ha='right',\n",
    "#                  va='bottom')\n",
    "\n",
    "#   plt.savefig(filename)\n",
    "\n",
    "# try:\n",
    "#   from sklearn.manifold import TSNE\n",
    "#   import matplotlib.pyplot as plt\n",
    "\n",
    "#   tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "#   plot_only = 500\n",
    "#   low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only,:])\n",
    "#   labels = [reverse_dictionary[i] for i in xrange(plot_only)]\n",
    "#   plot_with_labels(low_dim_embs, labels)\n",
    "\n",
    "# except ImportError:\n",
    "#   print(\"Please install sklearn and matplotlib to visualize embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCWMD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MC : Monte Carlo\n",
    "\n",
    "- Select the top k words for each rating type.  Take the frequencies of the top k words and convert the frequency to proportions.  Use the proportions for the top k words to generate a random review of length k that exclusively uses the top k words (randomly chosen according to their proportions)\n",
    "\n",
    "- Repeat the step above m times for each label to generate m centroids per label\n",
    "\n",
    "    - Two steps above are performed in centroidsCalc method\n",
    "    - Looking back I should have turned the the length of the review into a hyperparameter or converted it to the average review length of the given label\n",
    "\n",
    "\n",
    "- These randomly generated centroids will be used to classify test reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WMD : Word Mover Distance\n",
    "    \n",
    "- I prefer to think of Word Mover Distance in terms of Earth Mover's distance.  The inuition is identical just the structures being analyzed are different.\n",
    "    - Imagine you had two structures made out of legos, where all the blocks used are the same shape and same size but the blocks are of all different colors.  Earth Mover's distance calculates the minimum cost of converting the one lego structure into the other, where we are penalized when we use a lego that is a dhrastically different than the color of the block used in the same position in the other structure.\n",
    "    - now replace structures with reviews and blocks with words and we have Word Mover Distance.  \n",
    "    \n",
    "    \n",
    "- I first calculate the Distance Matrix(the penalty of converting one word into another) before predict is even called\n",
    "    - the penalty is the euclidean distance between the two words\n",
    "    - I calculate this matrix beforehand because we can only calculate distance for words we have embeddings for and the set of all embeddings is locked in as soon as the MCWMD class is instantiated.  By doing the calculation ahead of time the hope is to speed up predict time.\n",
    "        - I got the idea and intuition to do this from coding the PAM algorithm\n",
    "        \n",
    "        \n",
    "- Once Passed the reviews in the predict method:\n",
    "\n",
    "    I loop through reviews => \n",
    "    \n",
    "        then I loop through labels =>\n",
    "        \n",
    "            then I loop through the centroids for the label=>\n",
    "            \n",
    "                -then I CountVectorize the review and centroid and convert the frequencies to proportions\n",
    "                -I then grab the rows and columns of the distance matrix that contain the words used in \n",
    "                the review and centroid\n",
    "                -I finally calculate WMD for the given review and the given centroid for the given label\n",
    "                \n",
    "            I average the WMD for all the centroids for the given label\n",
    "            \n",
    "        I classify the review with the label that had the minimum average WMD\n",
    "        \n",
    "- As you can tell this procedure is very slow and could have been sped up if I had a better grasp on how to parrelize the outer loop step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyemd import emd\n",
    "import numpy as np \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict,Counter\n",
    "from itertools import chain\n",
    "\n",
    "class MCWMD:\n",
    "    def __init__(self,embeddings,embedDict,trainReviews,labels,distMetric,num_words,num_reviews=1,distMat = None):\n",
    "        self.embeddings = embeddings#word2vec output\n",
    "        self.embedDict = embedDict#dictionary maps words to index of embeddings\n",
    "        self.trainReviews = trainReviews # list of lists containing reviews\n",
    "        self.distMetric = distMetric#distance metric to be used to generate the distance\n",
    "        self.labels = labels #associated label for each training review\n",
    "        self.num_words = num_words#number of words to be used to generate the centroid\n",
    "        self.num_reviews = num_reviews#number of randomly generated centroids per label\n",
    "        self.centroids_calc(num_words)#generate the random centroids\n",
    "        if distMat is None:\n",
    "            self.distMat = dist_mat(embeddings,embedDict)#generate the distance matrix \n",
    "        else:\n",
    "            self.distMat = distMat#initialize the distance matrix \n",
    "\n",
    "    def centroids_calc(self,num_words):\n",
    "        \"\"\"calculates num_reviews centroids for k classes\"\"\"\n",
    "        indicies = xrange(len(self.labels))\n",
    "        numReviewsLst = xrange(self.num_reviews)\n",
    "\n",
    "        def flat_list(c):\n",
    "            #merge lists of reviews of the same label\n",
    "            labelC = self.labels == c\n",
    "            return c,list(chain(*[self.trainReviews[j] for i,j in zip(labelC,indicies) if i]))\n",
    "        \n",
    "        def avg_len(c):\n",
    "            #calculate the average review length for each label\n",
    "            labelC = self.labels == c\n",
    "            n = float(np.sum(labelC))\n",
    "            return round(int(sum([len(trainReviews[j]) for i,j in zip(labelC,indicies) if i])/n))\n",
    "        \n",
    "        classes = list(set(self.labels))#generate a list of the potential labels\n",
    "        classAvg = {c:avg_len(c) for c in classes} #calculate average length for each label\n",
    "        classList = list(map(flat_list,classes))#create a list of all the words used for each type of label\n",
    "        topWords  ={i:Counter(j).most_common(num_words) for i,j in classList}#create a dictionary where keys are the labels and values is \n",
    "        #a list of tuples of the most used words and their frequencies\n",
    "        \n",
    "        def break_up(i,j): return list(map(lambda x: x[i],topWords[j]))#function to capture either the word or proportions for a given label\n",
    "        centroidDict = {i:break_up(0,i) for i in classes}#capture the top words for each class\n",
    "        self.centroidPercents = {i:np.array(break_up(1,i)) for i in classes}#capture the frequency vector for the top words for each label\n",
    "        self.centroidPercents = {i:j/float(np.sum(j)) for i,j in self.centroidPercents.items()}#convert the \n",
    "        self.centroids = {i:list(map(lambda k: list(np.random.choice(j,size=num_words,p=self.centroidPercents[i])),numReviewsLst))\\\n",
    "                          for i,j in centroidDict.items()}#generate num_reviews random centroids for each label\n",
    "        \n",
    "        #self.centroids = {i:list(map(lambda k: list(np.random.choice(j,size=classAvg[i],p=self.centroidPercents[i])),numReviewsLst))\\\n",
    "        #                  for i,j in centroidDict.items()}#generate num_reviews random centroids for each label\n",
    "        \n",
    "        self.centroidStr = {i:list(map(lambda k:\" \".join(k),j)) for i,j in self.centroids.items()}# concatenate each \n",
    "        #randomly generated centroid into a string\n",
    "    \n",
    "    @staticmethod\n",
    "    def dist_mat(embeddings,embedDict,distMetric):\n",
    "        \"\"\"calculates distance matrix to be used in emd\n",
    "        calculate the distance between word embeddings between all words provided in the list\n",
    "        \"\"\"\n",
    "        return np.array(map(lambda i: distMetric(embeddings-i,axis=1),embeddings))#generate the distance matrix\n",
    "    \n",
    "    def calc_dist_mat(self,featNames):\n",
    "        \"\"\"calculates distance matrix to be used in emd\n",
    "        calculate the distance between word embeddings between all words provided in the list\n",
    "        \"\"\"\n",
    "        uniqueEmbedKeys = np.array([self.embedDict[i] for i in featNames])#get the index value for each word in featNames\n",
    "        uniqueDistMat = self.distMat[uniqueEmbedKeys]#select the unique rows that will be used for the emd calculation\n",
    "        return uniqueDistMat[:,uniqueEmbedKeys]#select the unique columns that will be used for the emd calculation\n",
    "    \n",
    "    def count_vectorizer(self,review,classIdx,reviewNumber):\n",
    "        \"\"\"CountVectorizer is used on the review and the randomly generated review for the given class label\n",
    "        CountVectorizer will produce a 2 x |unique vocab| matrix where each vector corresponds to a given\n",
    "        review and gives the distribution of words for the given review.  The list of the unique vocab is also\n",
    "        returned since it will be used in calcDistMat\"\"\"\n",
    "        allReviews = [\" \".join(review),self.centroidStr[classIdx][reviewNumber]]#generate a list of concatenated reviews used in the EMD calculation\n",
    "        cv = CountVectorizer(analyzer=lambda i: str(i).split())#initialize skLearn's CountVectorizer\n",
    "        arr = cv.fit_transform(allReviews)#generate the countvectorized matrix\n",
    "        probarr = arr.toarray().T/np.sum(arr.toarray(),axis=1,dtype=float)#convert the matrix entries to proportions\n",
    "        return probarr.T,cv.get_feature_names()#return proportion matrix and the corresponding feature names for each row\n",
    "    \n",
    "    def predict(self,reviews,cleanReviews=0):\n",
    "        \"\"\"takes all data(list of lists) and applies emd to each instance that is passed\n",
    "        reviews = list of lists - containing the reviews\n",
    "        cleanReviews = int - 0 if the each list does not contain just words that have embeddings\n",
    "        \"\"\"\n",
    "        classes = list(self.centroids.keys())\n",
    "        numReviewLst = range(self.num_reviews)\n",
    "        if cleanReviews==0:\n",
    "            reviews = map(lambda review: [j for j in review if j in self.embedDict.keys()],reviews)#eliminate words from our test reviews that we do not have word \n",
    "        #embeddings for\n",
    "        \n",
    "        def apply_emd(review):\n",
    "            \"\"\"take individual review and applies emd to each class's centroids and \n",
    "            returns label of centroid the review is closest to\"\"\"\n",
    "            def apply_emd_class(c):\n",
    "                \"\"\"take individual review and individual class and applies emd to each of the class's randomly\n",
    "                generated centroids and returns the mean distance between the given review and all of the class's\n",
    "                centroids\"\"\"\n",
    "                def centroid_emd(reviewNum):\n",
    "                    \"\"\"apply EMD to the review and one of the randomly generated centroids for a given class\n",
    "                    and returns the distance between the review and given centroid\"\"\"\n",
    "                    probArrays,featNames = self.count_vectorizer(review,c,reviewNum)#generate the distribution vectors \n",
    "                    #for the review and centroid\n",
    "                    distMatrix = self.calc_dist_mat(featNames)#generate the distance matrix for all the unique words\n",
    "                    #contained in centroid and review\n",
    "                    probArrays,distMatrix = np.array(probArrays,dtype=np.float64).copy(order='C'),np.array(distMatrix,dtype=np.float64).copy(order='C')\n",
    "                    #alter the format of the distribution vectors and distance matrix to that expected by the emd algorithm\n",
    "                    return emd(probArrays[0],probArrays[1],distMatrix)#calculate the minimum distance between the review and centroid\n",
    "                return np.mean(map(centroid_emd,numReviewLst)),c\n",
    "            return min(map(apply_emd_class,classes))[1]\n",
    "        return np.array(map(apply_emd,reviews))\n",
    "\n",
    "    def score(self,X,y):\n",
    "        \"\"\"calculates the accuracy MCWMD\"\"\"\n",
    "        n = float(len(X))\n",
    "        yHat = self.predict(X)\n",
    "        return np.sum(y==yHat)/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I tuned on the number of words chosen from to generate the random centroids and the number of random centroids to be generated.\n",
    "    - Like I said above I wish I would have made the length a hyperparameter or converted it to the average length for the given hyperparameter.\n",
    "    \n",
    "    \n",
    "- Remember I used the same data to train my model as I did to test it so there is overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resultDict = defaultdict(dict)\n",
    "numwordList = [50,100,200,500]\n",
    "centroidList = [2,5,10,20]\n",
    "def generateOutcome(numWords,numCentroids):\n",
    "    instance = MCWMD(embeddings,embedDict,topReviewLst,labels,np.linalg.norm,numWords,numCentroids,distMatrix,embedKeys)\n",
    "    tstOutput = instance.predict(topReviewLst,1)\n",
    "    resultDict[numWords][numCentroids]=tstOutput\n",
    "    print numWords,numCentroids\n",
    "    \n",
    "def applygenerateOutcome(i):\n",
    "    map(lambda j: generateOutcome(i,j),centroidList)\n",
    "    \n",
    "map(applygenerateOutcome,numwordList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50 words 2 centroids: 0.64\n",
    "\n",
    "50 words 5 centroids: 0.645\n",
    "\n",
    "50 words 10 centroids: 0.71\n",
    "\n",
    "50 words 20 centroids: 0.65\n",
    "\n",
    "100 words 2 centroids: 0.64\n",
    "\n",
    "100 words 5 centroids: 0.695\n",
    "\n",
    "100 words 10 centroids: 0.68\n",
    "\n",
    "100 words 20 centroids: 0.76\n",
    "\n",
    "200 words 2 centroids: 0.675\n",
    "\n",
    "200 words 5 centroids: 0.7\n",
    "\n",
    "200 words 10 centroids: 0.75\n",
    "\n",
    "200 words 20 centroids: 0.765\n",
    "\n",
    "500 words 2 centroids: 0.765\n",
    "\n",
    "500 words 5 centroids: 0.8\n",
    "\n",
    "500 words 10 centroids: 0.77\n",
    "\n",
    "500 words 20 centroids: 0.755"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~jwbaum91/16.embed\" height=\"525\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.tools as tls\n",
    "\n",
    "tls.embed(\"https://plot.ly/~jwbaum91/16/accuracy-of-hyperparameter-space/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Average Accuracy for Number of Words\n",
    "----\n",
    "- Is this telling us that more words in a centroid create a better classifier and/or longer centroids make for a better classifier? \n",
    "\n",
    "\n",
    "\n",
    "![](./Images/AvgNumWords.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Average Accuracy for Number of Centroids\n",
    "----\n",
    "- I found it suprising that number of centroids appears to not contribute that much to accuracy after 10 centroids\n",
    "\n",
    "\n",
    "\n",
    "![](./Images/AvgCentroid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "ROC Curves\n",
    "----\n",
    "\n",
    "![](./Images/ROCCurves.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative to Naive Bayes\n",
    "\n",
    "- used same train and test data as above to see how MCWMD does relative to a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes as nb\n",
    "nbClassifiers = [nb.MultinomialNB(),nb.BernoulliNB(),nb.GaussianNB()]\n",
    "def applyNB(model,Xtrn,Ytrn,Xtst,Ytst):\n",
    "    model.fit(Xtrn,Ytrn)\n",
    "    return model.predict(Xtst),model.score(Xtst,Ytst)\n",
    "\n",
    "nbOutput = list(map(lambda i:applyNB(i,nbTrain,labels,nbTrain,labels),nbClassifiers))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "NB ROC Curves\n",
    "----\n",
    "\n",
    "![](./Images/nb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Wish list\n",
    "----\n",
    "- MORE TIME\n",
    "\n",
    "\n",
    "- tsne plot\n",
    "\n",
    "\n",
    "- perform a test/train split\n",
    "\n",
    "\n",
    "\n",
    "- test length of centroid to the average length for the given label\n",
    "\n",
    "\n",
    "\n",
    "- parallelize algorithm\n",
    "\n",
    "\n",
    "- run algorithm without stop words\n",
    "\n",
    "\n",
    "- try more hyperparameter testing\n",
    "\n",
    "\n",
    "- tried algorithm on all 5 rating types\n",
    "\n",
    "\n",
    "- play around with different distance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Appendix\n",
    "\n",
    "I have annotated the pseudo code for Earth Mover's Distance and written the pseudo code in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.ariel.ac.il/sites/ofirpele/publications/ECCV2008.pdf\n",
    " \n",
    "EMD(Q, P, N):\n",
    "// Q is an array of probabilities for n features\n",
    "// P is an array of probabilities for n features\n",
    "// N is an integer, the length of P and Q\n",
    "    \n",
    "D ⇐ 0 // initialize the distance\n",
    "\n",
    "cQ ⇐ Q[0] //initialize the cost of Q with the first probability of Q, will hold the sum of Q's elements\n",
    "\n",
    "cP ⇐ P [0] // initialize the cost of P with the first probability of P, will hold the sum of P's elements\n",
    "\n",
    "F[0] ⇐ Q[0] − P[0] //set the first element of the array F equal to the difference in probabilities between Q and P at point 0\n",
    "\n",
    "fori=1toN−1do// loop through the remaining elements of P and Q\n",
    "\n",
    "    cQ ⇐ cQ + Q[i] //sum up all the elements of Q and set it equal to cQ\n",
    "    cP ⇐ cP + P [i] // sum up all the elements of P and set it equal to cP\n",
    "    F [i] ⇐ cQ − cP //have the ith element of F hold the difference between the sum of array Q up to element i(inclucsively) and the sum of array P up to element i(inclucsively)\n",
    "\n",
    "i⇐((index of the median of F)+1) mod N // extract the index of the median, don't know why perform mod n\n",
    "\n",
    "for t=0 to N−1 do //loop through N times\n",
    "\n",
    "    I[t] ⇐ i // starting with the median index set I equal to the index of the median\n",
    "    //the N-1 element of I will be the median index minus 1\n",
    "    i⇐(i+1) modN // increase i by 1 and make sure i is between 0 and N-1\n",
    "    \n",
    "tQ⇐0 // set tQ equal to 0\n",
    "\n",
    "tP ⇐ 0 // set tQ equal to 0\n",
    "\n",
    "iQ ⇐ I[tQ] //set iQ equal to the index of the median\n",
    "\n",
    "iP ⇐I[tP] //set iP equal to the index of the median\n",
    "\n",
    "while true do // loop until you hit one of the two terminating conditions which are indicated with a *\n",
    "\n",
    "    while Q[iQ]=0 do // if the current element of Q equals zero keep looping until you find the next \n",
    "    //nonzero value of Q or if you have looped through all values of Q return the distance\n",
    "        tQ ⇐ tQ + 1\n",
    "        if tQ=N then // * terminating condition where we have looped throug all the indicies of Q\n",
    "            return D // return the distance\n",
    "        iQ ⇐ I[tQ]\n",
    "    while P[iP]=0 do // if the current element of P equals zero keep looping until you find the next \n",
    "    //nonzero value of P or if you have looped through all values of P return the distance\n",
    "        tP ⇐ tP + 1\n",
    "        if tP =N then // * terminating condition where we have looped throug all the indicies of P\n",
    "            return D // return the distance\n",
    "        iP ⇐I[tP]\n",
    "\n",
    "    f ⇐ min(Q[iQ], P [iP ])//take the minimum value between the nonzero value of Q and nonzero value of P\n",
    "    Q[iQ] ⇐ Q[iQ] − f // either set to zero or Q[iQ]-P[iP ] which is greater than zero\n",
    "    P[iP] ⇐ P[iP] − f // either set to zero or P[iP ]-Q[iQ] which is greater than zero\n",
    "    D ⇐ f × min(|iQ − iP |, N − |iQ − iP |) // take the minimum proportion and multiply it by the amount of how far P and Q's index are from one another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def emd(Q,P):\n",
    "    \"\"\"implementation of the above pseudocode in Python\n",
    "    accepts two arrays Q and P of equal length N\n",
    "    the function calculates Earth Mover's Distance between Q and P using L1 loss\n",
    "    the function returns Earth Mover's Distance between the two arrays\"\"\"\n",
    "    def incrementSum(a):\n",
    "        \"\"\"accepts an array and returns an array of equal length\n",
    "        where element i of the outputed array is the sum of the elements\n",
    "        of the inputed array up to index i, inclusively\"\"\"\n",
    "        return np.array(map(lambda i: sum(a[:i]),xrange(len(a))))\n",
    "    \n",
    "    N = len(Q)\n",
    "    D = 0#initialize our EMD value\n",
    "    cQ = incrementSum(Q)#generate our cummulative sum vector for Q\n",
    "    cP = incrementSum(P)#generate our cummulative sum vector for P\n",
    "    F = cQ - cP#take the difference between our two cummulative sum vectors\n",
    "    \n",
    "    i = np.where(F==np.percentile(F,50,interpolation='nearest'))[0][0]#find the index of the median value of \n",
    "    #our differences in cummulative sums\n",
    "    I = [(j+i)%N for j in xrange(N)]#generate an array that goes from the index of the median to the index of the\n",
    "    # median -1 and all values are bounded [0,N-1]\n",
    "    tQ,tP = 0,0\n",
    "    iQ,iP = I[tQ],I[tP]#initialize iQ and iP to the index of Q and P that generated the median \n",
    "    #difference in cummulative sums\n",
    "    \n",
    "    while True:#loop until one of the two break conditions is hit denoted with *\n",
    "        while Q[iQ]==0:#loop until the current value of Q is nonzero\n",
    "            tQ+=1\n",
    "            if tQ==N:# * if you have looped through all the elements in Q\n",
    "                return D#return the distance\n",
    "            iQ = I[tQ]\n",
    "            \n",
    "        while P[iP]==0:#loop until the current value of P is nonzero\n",
    "            tP +=1\n",
    "            if tP==N:# * if you have looped through all the elements in P\n",
    "                return D#return the distance\n",
    "            iP = I[tP]\n",
    "        f = min(Q[iQ],P[iP])#set f equal to the minimum proportion\n",
    "        Q[iQ]-=f#either set to zero or Q[iQ]-P[iP ] which is greater than zero\n",
    "        P[iP]-=f#either set to zero or P[iP ]-Q[iQ] which is greater than zero\n",
    "        D += f*min(abs(iQ-iP),N-abs(iQ-iP))#take the minimum proportion and \n",
    "        #multiply it by the amount of how far P and Q's index are from one another\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
